{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: torch==2.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/prithvipandey/Library/Python/3.11/lib/python/site-packages (from torch==2.6.0->torchvision) (4.13.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AudioLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=1, hidden_size=256, batch_first=True, dropout=0.2, num_layers=1)\n",
    "        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, batch_first=True, dropout=0.2, num_layers=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]  # Get last time step\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        return x  # feature vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageResNet, self).__init__()\n",
    "        base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # Remove FC layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x.view(x.size(0), -1)  # Flatten (B, 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load FER2013\n",
    "fer_df = pd.read_csv('/Users/prithvipandey/Documents/dataset/fer2013.csv')\n",
    "\n",
    "# Filter only 'Training' data\n",
    "train_fer = fer_df[fer_df['Usage'] == 'Training']\n",
    "\n",
    "# Transform\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Convert pixel strings to image tensors\n",
    "fer_images = []\n",
    "fer_labels = []\n",
    "\n",
    "for idx, row in train_fer.iterrows():\n",
    "    pixels = list(map(int, row['pixels'].split()))\n",
    "    img = Image.fromarray(\n",
    "        torch.tensor(pixels).reshape(48, 48).numpy().astype('uint8')\n",
    "    )\n",
    "    img_tensor = image_transform(img)\n",
    "    fer_images.append(img_tensor)\n",
    "    fer_labels.append(int(row['emotion']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TESS MFCC features to tess_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to TESS dataset folder\n",
    "tess_path = '/Users/prithvipandey/Downloads/archive/TESS Toronto emotional speech set data'\n",
    "\n",
    "# Emotions to keep (mapping for FER2013 if needed)\n",
    "emotion_map = {\n",
    "    'angry': 'angry',\n",
    "    'disgust': 'disgust',\n",
    "    'fear': 'fear',\n",
    "    'happy': 'happy',\n",
    "    'neutral': 'neutral',\n",
    "    'ps': 'surprise',  # Sometimes \"pleasant surprise\" is written as \"ps\"\n",
    "    'sad': 'sad'\n",
    "}\n",
    "\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(tess_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            emotion_label = None\n",
    "            for key in emotion_map:\n",
    "                if key in file.lower():\n",
    "                    emotion_label = emotion_map[key]\n",
    "                    break\n",
    "            if emotion_label is None:\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Load audio\n",
    "            try:\n",
    "                y, sr = librosa.load(file_path, sr=None)\n",
    "                # Extract MFCCs\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "                mfcc_mean = np.mean(mfcc.T, axis=0)  # Average across time axis\n",
    "                row = [file_path, emotion_label] + mfcc_mean.tolist()\n",
    "                data.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "columns = ['filename', 'emotion'] + [f'mfcc_{i}' for i in range(40)]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('tess_features.csv', index=False)\n",
    "\n",
    "print(\"Saved TESS MFCC features to tess_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load audio features from CSV\n",
    "tess_df = pd.read_csv('tess_features.csv')\n",
    "\n",
    "# Map emotion strings to same label format as FER2013 (0–6 or 0–7)\n",
    "emotion_map = {\n",
    "    'angry': 0,\n",
    "    'disgust': 1,\n",
    "    'fear': 2,\n",
    "    'happy': 3,\n",
    "    'sad': 4,\n",
    "    'surprise': 5,\n",
    "    'neutral': 6\n",
    "}\n",
    "# Filter only matching emotions\n",
    "tess_df = tess_df[tess_df['emotion'].isin(emotion_map.keys())]\n",
    "\n",
    "audio_features = []\n",
    "audio_labels = []\n",
    "\n",
    "for _, row in tess_df.iterrows():\n",
    "    # Select only numeric columns for features\n",
    "    features = row.drop(['filename', 'emotion']).values.astype(np.float32)\n",
    "    tensor = torch.tensor(features)\n",
    "    audio_features.append(tensor)\n",
    "    audio_labels.append(emotion_map[row['emotion']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, image_data, audio_data, labels):\n",
    "        self.image_data = image_data\n",
    "        self.audio_data = audio_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.image_data), len(self.audio_data), len(self.labels))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_data[idx], self.audio_data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine only matching label data\n",
    "min_len = min(len(fer_labels), len(audio_labels))\n",
    "fusion_images = fer_images[:min_len]\n",
    "fusion_audio = audio_features[:min_len]\n",
    "fusion_labels = fer_labels[:min_len]  # assuming same mapping as audio\n",
    "\n",
    "# Create dataset\n",
    "fusion_dataset = FusionDataset(fusion_images, fusion_audio, fusion_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(fusion_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalFusion, self).__init__()\n",
    "        self.image_model = ImageResNet()\n",
    "        self.audio_model = AudioLSTM()\n",
    "\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(2048 + 64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 7)  # Or 8 depending on your target class setup\n",
    "        )\n",
    "\n",
    "    def forward(self, image_input, audio_input):\n",
    "        img_feat = self.image_model(image_input)\n",
    "        aud_feat = self.audio_model(audio_input)\n",
    "        combined = torch.cat((img_feat, aud_feat), dim=1)\n",
    "        output = self.fusion_fc(combined)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion_model(model, dataloader, optimizer, loss_fn, device, num_epochs=10):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for image_inputs, audio_inputs, labels in dataloader:\n",
    "            image_inputs = image_inputs.to(device)\n",
    "            audio_inputs = audio_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image_inputs, audio_inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")\n",
    "\n",
    "    return train_losses, train_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mMultimodalFusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      2\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m3e-4\u001b[39m)\n\u001b[32m      3\u001b[39m loss_fn = nn.CrossEntropyLoss()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mMultimodalFusion.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m(MultimodalFusion, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.image_model = \u001b[43mImageResNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.audio_model = AudioLSTM()\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m.fusion_fc = nn.Sequential(\n\u001b[32m      8\u001b[39m         nn.Linear(\u001b[32m2048\u001b[39m + \u001b[32m64\u001b[39m, \u001b[32m256\u001b[39m),\n\u001b[32m      9\u001b[39m         nn.ReLU(),\n\u001b[32m     10\u001b[39m         nn.Dropout(\u001b[32m0.3\u001b[39m),\n\u001b[32m     11\u001b[39m         nn.Linear(\u001b[32m256\u001b[39m, \u001b[32m7\u001b[39m)  \u001b[38;5;66;03m# Or 8 depending on your target class setup\u001b[39;00m\n\u001b[32m     12\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mImageResNet.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m(ImageResNet, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     base_model = \u001b[43mmodels\u001b[49m.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.feature_extractor = nn.Sequential(*\u001b[38;5;28mlist\u001b[39m(base_model.children())[:-\u001b[32m1\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "model = MultimodalFusion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "train_losses, train_accuracies = train_fusion_model(\n",
    "    model, train_loader, optimizer, loss_fn, device, num_epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ImageResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageResNet, self).__init__()\n",
    "        base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "class AudioLSTM(nn.Module):\n",
    "  #...your audio LSTM code...\n",
    "    def __init__(self):\n",
    "        super(AudioLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=40, hidden_size=32, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(32 * 2, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the last time step output\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "class MultimodalFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalFusion, self).__init__()\n",
    "        self.image_model = ImageResNet()\n",
    "        self.audio_model = AudioLSTM()\n",
    "\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(2048 + 64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 7)  # Or 8 depending on your target class setup\n",
    "        )\n",
    "\n",
    "    def forward(self, image_input, audio_input):\n",
    "        image_features = self.image_model(image_input)\n",
    "        audio_features = self.audio_model(audio_input)\n",
    "        combined_features = torch.cat((image_features, audio_features), dim=1)\n",
    "        output = self.fusion_fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Example usage (assuming you have 'device' defined)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalFusion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Example input.\n",
    "image_input = torch.randn(1,3,224,224).to(device)\n",
    "audio_input = torch.randn(1,10,40).to(device)\n",
    "\n",
    "output = model(image_input,audio_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "torch.Size([1, 7])\n",
      "Example loss: 2.013303756713867\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Determine the device (MPS if available, otherwise CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ImageResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageResNet, self).__init__()\n",
    "        base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "class AudioLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=40, hidden_size=32, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(32 * 2, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the last time step output\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "class MultimodalFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalFusion, self).__init__()\n",
    "        self.image_model = ImageResNet()\n",
    "        self.audio_model = AudioLSTM()\n",
    "\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(2048 + 64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 7)  # Or 8 depending on your target class setup\n",
    "        )\n",
    "\n",
    "    def forward(self, image_input, audio_input):\n",
    "        image_features = self.image_model(image_input)\n",
    "        audio_features = self.audio_model(audio_input)\n",
    "        combined_features = torch.cat((image_features, audio_features), dim=1)\n",
    "        output = self.fusion_fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model and move it to the MPS device\n",
    "model = MultimodalFusion().to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Example input (move inputs to the MPS device)\n",
    "image_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "audio_input = torch.randn(1, 10, 40).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = model(image_input, audio_input)\n",
    "print(output.shape)\n",
    "\n",
    "# Example of a loss calculation.\n",
    "target = torch.randint(0, 7, (1,)).to(device) # Example target\n",
    "loss = loss_fn(output, target)\n",
    "print(f\"Example loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Use MPS on Mac if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load FER2013 CSV (Image Data)\n",
    "fer_df = pd.read_csv(\"/Users/prithvipandey/Documents/dataset/fer2013.csv\")\n",
    "fer_df = fer_df[fer_df['Usage'] == 'Training']\n",
    "fer_images = fer_df['pixels'].apply(lambda x: np.fromstring(x, sep=' ').reshape(48, 48).astype(np.uint8))\n",
    "fer_labels = fer_df['emotion'].values\n",
    "\n",
    "# Resize & preprocess images for ResNet\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fer_image_tensors = torch.stack([image_transform(img) for img in fer_images])\n",
    "fer_labels = torch.tensor(fer_labels)\n",
    "\n",
    "# Load TESS CSV (Audio Features)\n",
    "tess_df = pd.read_csv(\"tess_features.csv\")  # CSV with MFCC or other audio features\n",
    "tess_features = tess_df.drop(\"label\", axis=1).values.reshape(-1, 10, 40).astype(np.float32)\n",
    "tess_labels = tess_df['label'].values\n",
    "\n",
    "# Encode TESS labels to numeric to match FER2013 (map emotions to same index!)\n",
    "emotion_map = {'neutral': 0, 'happy': 1, 'sad': 2, 'angry': 3, 'fear': 4, 'disgust': 5, 'surprise': 6}\n",
    "tess_labels = np.array([emotion_map[label] for label in tess_labels])\n",
    "\n",
    "# Ensure same length by trimming or balancing\n",
    "min_len = min(len(fer_labels), len(tess_labels))\n",
    "fer_image_tensors = fer_image_tensors[:min_len]\n",
    "fer_labels = fer_labels[:min_len]\n",
    "tess_features = tess_features[:min_len]\n",
    "tess_labels = tess_labels[:min_len]\n",
    "\n",
    "# Final labels must match for multimodal fusion\n",
    "assert all(fer_labels.numpy() == tess_labels)\n",
    "\n",
    "# Train/Val Split\n",
    "X_img_train, X_img_val, X_aud_train, X_aud_val, y_train, y_val = train_test_split(\n",
    "    fer_image_tensors, tess_features, fer_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Torch Dataset\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_data, audio_data, labels):\n",
    "        self.image_data = image_data\n",
    "        self.audio_data = torch.tensor(audio_data)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_data[idx], self.audio_data[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = MultimodalDataset(X_img_train, X_aud_train, y_train)\n",
    "val_dataset = MultimodalDataset(X_img_val, X_aud_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Image model: ResNet-50\n",
    "class ImageResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageResNet, self).__init__()\n",
    "        base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "# Audio model: BiLSTM\n",
    "class AudioLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=40, hidden_size=32, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(64, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Fusion Model\n",
    "class MultimodalFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalFusion, self).__init__()\n",
    "        self.image_model = ImageResNet()\n",
    "        self.audio_model = AudioLSTM()\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(2048 + 64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 7)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_input, audio_input):\n",
    "        img_feat = self.image_model(image_input)\n",
    "        aud_feat = self.audio_model(audio_input)\n",
    "        combined = torch.cat((img_feat, aud_feat), dim=1)\n",
    "        return self.fusion_fc(combined)\n",
    "\n",
    "# Training setup\n",
    "model = MultimodalFusion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss, preds, targets = 0, [], []\n",
    "    for img, aud, label in train_loader:\n",
    "        img, aud, label = img.to(device), aud.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img, aud)\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "        targets.extend(label.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average='weighted')\n",
    "    print(f\"Epoch {epoch+1} | Loss: {train_loss/len(train_loader):.4f} | Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for img, aud, label in val_loader:\n",
    "            img, aud, label = img.to(device), aud.to(device), label.to(device)\n",
    "            output = model(img, aud)\n",
    "            val_preds.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "            val_targets.extend(label.cpu().numpy())\n",
    "    val_acc = accuracy_score(val_targets, val_preds)\n",
    "    val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Use MPS on Mac if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load FER2013 CSV (Image Data)\n",
    "fer_df = pd.read_csv(\"/Users/prithvipandey/Documents/dataset/fer2013.csv\")\n",
    "fer_df = fer_df[fer_df['Usage'] == 'Training']\n",
    "fer_images = fer_df['pixels'].apply(lambda x: np.fromstring(x, sep=' ').reshape(48, 48).astype(np.uint8))\n",
    "fer_labels = fer_df['emotion'].values\n",
    "\n",
    "# Resize & preprocess images for ResNet\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fer_image_tensors = torch.stack([image_transform(img) for img in fer_images])\n",
    "fer_labels = torch.tensor(fer_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
